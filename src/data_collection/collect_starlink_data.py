# -*- coding: utf-8 -*-
"""collect_starlink_data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RwfH1AfMZ6-PYecS5KdgMAjHdfdK6r69
"""

#!/usr/bin/env python3
import subprocess
import datetime
import time
import os
import signal
import sys
import logging
import sqlite3
from typing import Dict

class DataCollector:
    def __init__(self):
        self.data_dir = "collected_data"
        self.log_dir = f"{self.data_dir}/logs"
        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        self.processes: Dict[str, subprocess.Popen] = {}

        for directory in [self.data_dir, self.log_dir]:
            if not os.path.exists(directory):
                os.makedirs(directory)

        self.setup_logging()
        self.collection_start_time = datetime.datetime.now()
        self.metrics = {
            'csv': {'cycles': 0, 'failures': 0, 'last_success': None},
            'sqlite': {'records': 0, 'cycles': 0, 'failures': 0, 'last_success': None}
        }

        self.csv_file = f"{self.data_dir}/starlink_data_{self.timestamp}.csv"
        self.db_file = f"{self.data_dir}/starlink_data_{self.timestamp}.db"
        self.process_active = {'csv': False, 'sqlite': False}
        self.last_sqlite_count = 0
        self.sqlite_cycle_start = None

    def setup_logging(self):
        log_file = f"{self.log_dir}/collection_{self.timestamp}.log"
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )

    def start_csv_collection(self) -> bool:
        if self.process_active['csv']:
            return True

        cmd = [
            "python3", "dish_grpc_text.py",
            "-t", "30",
            "-H",
            "-O", self.csv_file,
            "status",
            "obstruction_detail",
            "alert_detail",
            "ping_drop",
            "ping_latency"
        ]

        try:
            logging.debug(f"Starting CSV process with command: {' '.join(cmd)}")
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            self.processes['csv'] = process
            self.process_active['csv'] = True
            logging.info(f"Started CSV collection to {self.csv_file}")
            return True
        except Exception as e:
            logging.error(f"Failed to start CSV collection: {e}")
            logging.debug(f"CSV start exception details:", exc_info=True)
            return False

    def start_sqlite_collection(self) -> bool:
        if self.process_active['sqlite']:
            return True

        cmd = [
            "python3", "dish_grpc_sqlite.py",
            "-v",
            "-t", "30",
            self.db_file,
            "status"
        ]

        try:
            logging.debug(f"Starting SQLite process with command: {' '.join(cmd)}")
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )

            time.sleep(1)
            if process.poll() is not None:
                _, stderr = process.communicate()
                if stderr:
                    logging.error(f"SQLite process failed to start: {stderr}")
                    return False

            self.processes['sqlite'] = process
            self.process_active['sqlite'] = True
            self.sqlite_cycle_start = datetime.datetime.now()
            logging.info(f"Started SQLite collection to {self.db_file}")

            return True
        except Exception as e:
            logging.error(f"Failed to start SQLite collection: {e}")
            logging.debug(f"SQLite start exception details:", exc_info=True)
            return False

    def check_sqlite_progress(self) -> bool:
        """Check if SQLite database is growing"""
        try:
            conn = sqlite3.connect(self.db_file)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM status")
            current_count = cursor.fetchone()[0]
            conn.close()

            now = datetime.datetime.now()
            if current_count > self.last_sqlite_count:
                new_records = current_count - self.last_sqlite_count
                self.metrics['sqlite']['records'] += new_records
                self.metrics['sqlite']['last_success'] = now

                # Track completion of a full cycle (30 seconds)
                elapsed = (now - self.sqlite_cycle_start).total_seconds() if self.sqlite_cycle_start else 0
                if elapsed >= 29.5:  # Allow small timing variations
                    self.metrics['sqlite']['cycles'] += 1
                    self.sqlite_cycle_start = now
                    logging.debug(f"SQLite cycle {self.metrics['sqlite']['cycles']} completed")

                self.last_sqlite_count = current_count
                logging.debug(f"SQLite records increased by {new_records} to {current_count}")
                return True

            return False

        except Exception as e:
            logging.error(f"Error checking SQLite progress: {e}")
            return False

    def check_processes(self) -> Dict[str, bool]:
        needs_restart = {'csv': False, 'sqlite': False}

        # Check SQLite progress
        if not self.check_sqlite_progress():
            needs_restart['sqlite'] = True
            self.metrics['sqlite']['failures'] += 1

        # Check CSV process
        if 'csv' in self.processes:
            process = self.processes['csv']
            if process.poll() is not None:
                retcode = process.poll()
                stdout, stderr = process.communicate()

                if stdout:
                    logging.debug(f"CSV stdout: {stdout}")
                if stderr:
                    logging.debug(f"CSV stderr: {stderr}")

                if retcode != 0:
                    self.metrics['csv']['failures'] += 1
                    logging.error(f"CSV collection failed: {stderr}")
                    needs_restart['csv'] = True
                else:
                    self.metrics['csv']['cycles'] += 1
                    self.metrics['csv']['last_success'] = datetime.datetime.now()
                    logging.info(f"CSV collection cycle completed")

                self.process_active['csv'] = False

        return needs_restart

    def log_metrics(self):
        logging.info("Collection Metrics:")
        duration = datetime.datetime.now() - self.collection_start_time
        duration_seconds = duration.total_seconds()
        logging.info(f"Total duration: {duration}")

        logging.info(f"CSV Statistics:")
        logging.info(f"  Collection cycles completed: {self.metrics['csv']['cycles']}")
        cycle_rate = self.metrics['csv']['cycles'] / (duration_seconds / 60) if duration_seconds > 0 else 0
        logging.info(f"  Average cycles per minute: {int(round(cycle_rate))}")
        success_rate = 100 * (self.metrics['csv']['cycles'] / (self.metrics['csv']['cycles'] + self.metrics['csv']['failures'])) if self.metrics['csv']['cycles'] + self.metrics['csv']['failures'] > 0 else 100
        logging.info(f"  Success/Failure: {self.metrics['csv']['cycles']}/{self.metrics['csv']['failures']} ({success_rate:.1f}% success)")
        if self.metrics['csv']['last_success']:
            last_success_age = datetime.datetime.now() - self.metrics['csv']['last_success']
            logging.info(f"  Time since last success: {last_success_age}")

        logging.info(f"SQLite Statistics:")
        logging.info(f"  Collection cycles completed: {self.metrics['sqlite']['cycles']}")
        logging.info(f"  Total records collected: {self.metrics['sqlite']['records']}")
        record_rate = self.metrics['sqlite']['records'] / (duration_seconds / 60) if duration_seconds > 0 else 0
        logging.info(f"  Average records per minute: {int(round(record_rate))}")
        success_rate = 100 * (self.metrics['sqlite']['cycles'] / (self.metrics['sqlite']['cycles'] + self.metrics['sqlite']['failures'])) if self.metrics['sqlite']['cycles'] + self.metrics['sqlite']['failures'] > 0 else 100
        logging.info(f"  Success/Failure: {self.metrics['sqlite']['cycles']}/{self.metrics['sqlite']['failures']} ({success_rate:.1f}% success)")
        if self.metrics['sqlite']['last_success']:
            last_success_age = datetime.datetime.now() - self.metrics['sqlite']['last_success']
            logging.info(f"  Time since last success: {last_success_age}")

    def cleanup(self):
        logging.info("Starting cleanup...")
        for process_type, process in self.processes.items():
            try:
                if process and process.poll() is None:
                    process.terminate()
                    process.wait(timeout=5)
                    logging.debug(f"Successfully terminated {process_type} process")
            except Exception as e:
                logging.error(f"Error terminating {process_type} process: {e}")
                try:
                    process.kill()
                    logging.debug(f"Killed {process_type} process after terminate failed")
                except Exception as e:
                    logging.error(f"Error killing {process_type} process: {e}")

        self.log_metrics()
        logging.info("Cleanup completed")

    def start_collection(self):
        logging.info(f"Starting data collection at {self.timestamp}")
        logging.info(f"Data will be stored in {self.data_dir}")

        if not self.start_csv_collection() or not self.start_sqlite_collection():
            logging.error("Failed to start data collection")
            self.cleanup()
            return

        try:
            while True:
                needs_restart = self.check_processes()

                for process_type in ['csv', 'sqlite']:
                    if needs_restart[process_type] or not self.process_active[process_type]:
                        if needs_restart[process_type]:
                            logging.info(f"Restarting {process_type} collection due to failure...")
                        else:
                            logging.info(f"Starting {process_type} collection...")

                        if process_type == 'csv':
                            self.start_csv_collection()
                        else:
                            self.start_sqlite_collection()

                time.sleep(30)

        except KeyboardInterrupt:
            logging.info("\nStopping data collection...")
            self.cleanup()
        except Exception as e:
            logging.error(f"Unexpected error: {e}")
            logging.debug("Collection error details:", exc_info=True)
            self.cleanup()

def signal_handler(signum, frame):
    logging.info(f"Received signal {signum}")
    collector.cleanup()
    sys.exit(0)

if __name__ == "__main__":
    collector = DataCollector()
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    collector.start_collection()